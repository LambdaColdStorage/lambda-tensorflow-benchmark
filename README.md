
This is the code used for a few of the blog posts on: https://lambdalabs.com/blog including:

- 2080 Ti Deep Learning Benchmarks: https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/

Environment:
- OS: Ubuntu 18.04
- TensorFlow version: 1.14.0
- CUDA Version 10.0
- CUDNN Version 7.6.2

#### Step One: Clone benchmark repo


```
git clone https://github.com/lambdal/lambda-tensorflow-benchmark.git
```

#### Step Two: Run benchmark with thermal profile

```
TF_XLA_FLAGS=--tf_xla_auto_jit=2 ./batch_benchmark.sh min_num_gpus max_num_gpus num_runs num_batches_per_run thermal_sampling_frequency
python display_thermal.py path-to-thermal.log --thermal_threshold

# example of benchmarking 4 2080_Ti (all used), 1 run, 200 batches per run, measuring thermal every 2 second. 2080_Ti throttles at 89 C.
TF_XLA_FLAGS=--tf_xla_auto_jit=2 ./batch_benchmark.sh 4 4 1 200 2
python display_thermal.py i9-7920X-GeForce_RTX_2080_Ti.logs/resnet152-syn-replicated-fp32-4gpus-32-1-thermal.log --thermal_threshold 89

```

#### Note

Use large num_batches_per_run for a thorough test.


<!-- #### Step Two: Run benchmark

* Input proper gpu_indices (a comma seperated list, default 0) and num_iterations (default 10)
```
cd lambda-tensorflow-benchmark
./benchmark.sh gpu_indices num_iterations
```

#### Step Three: Report results

* Check the repo directory for folder \<cpu>-\<gpu>.logs (generated by benchmark.sh)
* Use the same num_iterations and gpu_indices for both benchmarking and reporting.
```
./report.sh <cpu>-<gpu>.logs
```

#### Batch process:

```
TF_XLA_FLAGS=--tf_xla_auto_jit=2 ./batch_benchmark.sh min_num_gpus max_num_gpus num_iterations

./report.sh <cpu>-<gpu>.logs

./gether.sh
```

 -->
